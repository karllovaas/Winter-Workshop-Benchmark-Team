{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2893ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# Access to SEC, URLs, and Revenue & Shares tags\n",
    "# ----------------------------\n",
    "\n",
    "#This is to access the API, otherwise the request will be denied\n",
    "SEC_HEADERS = {\n",
    "    \"User-Agent\": \"Maria Esparza (maria.esparza2949@gmail.com)\",  \n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "}\n",
    "\n",
    "TICKER_CIK_URL = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "COMPANY_FACTS_URL = \"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "\n",
    "REVENUE_TAGS = [\n",
    "    \"Revenues\",\n",
    "    \"SalesRevenueNet\",\n",
    "    \"RevenueFromContractWithCustomerExcludingAssessedTax\",\n",
    "]\n",
    "\n",
    "SHARES_TAG = \"CommonStockSharesOutstanding\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0320283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------\n",
    "\n",
    "'''\n",
    "Get a JSON payload from SEC-friendly endpoints\n",
    "'''\n",
    "def fetch_json(url: str, headers: dict, sleep_s: float = 0.2) -> dict:\n",
    "    r = requests.get(url, headers=headers, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code} for {url}: {r.text[:200]}\")\n",
    "    data = r.json()\n",
    "    time.sleep(sleep_s)  # SEC courtesy delay\n",
    "    return data\n",
    "\n",
    "'''\n",
    "SEC uses CIK values in place of tickers. \n",
    "We pull a ticker -> CIK mapping from the SEC.\n",
    "Return DataFrame: ticker (uppercase), cik (10-digit zero-padded string).\n",
    "'''\n",
    "def load_ticker_cik_map(headers: dict = SEC_HEADERS) -> pd.DataFrame:\n",
    "    raw = fetch_json(TICKER_CIK_URL, headers={\"User-Agent\": headers[\"User-Agent\"]})\n",
    "    rows = []\n",
    "    for _, v in raw.items():\n",
    "        ticker = v[\"ticker\"].upper()\n",
    "        cik = str(v[\"cik_str\"]).zfill(10)\n",
    "        rows.append((ticker, cik))\n",
    "    return pd.DataFrame(rows, columns=[\"ticker\", \"cik\"])\n",
    "\n",
    "'''\n",
    "Fetch companyfacts JSON for a 10-digit CIK string.\n",
    "'''\n",
    "def get_companyfacts(cik: str, headers: dict = SEC_HEADERS) -> dict:\n",
    "    url = COMPANY_FACTS_URL.format(cik=cik)\n",
    "    return fetch_json(url, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "375662c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Tag selection + extraction\n",
    "# ----------------------------\n",
    "\n",
    "'''\n",
    "Return (unique_fy_count, df_of_units) for a tag+unit, or (0, None) if missing.\n",
    "'''\n",
    "def fy_coverage_for_tag(usgaap: dict, tag: str, unit: str = \"USD\") -> Tuple[int, Optional[pd.DataFrame]]:\n",
    "    if tag not in usgaap:\n",
    "        return 0, None\n",
    "    units = usgaap[tag].get(\"units\", {})\n",
    "    if unit not in units:\n",
    "        return 0, None\n",
    "    df = pd.DataFrame(units[unit])\n",
    "    if \"fy\" not in df.columns:\n",
    "        return 0, df\n",
    "    return df[\"fy\"].nunique(dropna=True), df\n",
    "\n",
    "'''\n",
    "Pick the revenue tag with the best FY coverage.\n",
    "'''\n",
    "def choose_best_revenue_tag(usgaap: dict, tags: List[str] = REVENUE_TAGS, unit: str = \"USD\") -> Tuple[Optional[str], Optional[pd.DataFrame]]:\n",
    "    best_tag, best_df, best_score = None, None, -1\n",
    "    for tag in tags:\n",
    "        score, df_tmp = fy_coverage_for_tag(usgaap, tag, unit=unit)\n",
    "        if score > best_score and df_tmp is not None:\n",
    "            best_score = score\n",
    "            best_tag = tag\n",
    "            best_df = df_tmp\n",
    "    return best_tag, best_df\n",
    "\n",
    "'''\n",
    "Clean an SEC 'units' dataframe into one row per fiscal year:\n",
    "    - filter to annual forms\n",
    "    - (optionally) filter to fp == 'FY' if available\n",
    "    - dedupe by keeping latest end date per FY\n",
    "Returns columns: fy, end, val\n",
    "'''\n",
    "def clean_annual_series(raw_df: pd.DataFrame, unit_name: str, prefer_forms=(\"10-K\", \"10-K/A\")) -> pd.DataFrame:\n",
    "    df = raw_df.copy()\n",
    "    \n",
    "    # keep annual filings\n",
    "    if \"form\" in df.columns:\n",
    "        df = df[df[\"form\"].isin(prefer_forms)]\n",
    "\n",
    "    # keep full-year if fp column exists and FY exists\n",
    "    if \"fp\" in df.columns and (df[\"fp\"] == \"FY\").any():\n",
    "        df = df[df[\"fp\"] == \"FY\"]\n",
    "\n",
    "    # ensure the core columns exist\n",
    "    for col in [\"fy\", \"end\", \"val\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing column {col} in SEC facts for {unit_name}\")\n",
    "\n",
    "    df = df.dropna(subset=[\"fy\", \"end\", \"val\"])\n",
    "\n",
    "    df[\"end\"] = pd.to_datetime(df[\"end\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"end\"])\n",
    "    \n",
    "    # filed might be missing; handle gracefully\n",
    "    if \"filed\" in df.columns:\n",
    "        df[\"filed\"] = pd.to_datetime(df[\"filed\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"filed\"] = pd.NaT\n",
    "\n",
    "    # Keep the latest observation per FY (using end date)\n",
    "    df = (\n",
    "        df.sort_values([\"fy\", \"end\"])\n",
    "          .drop_duplicates(subset=[\"fy\"], keep=\"last\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df[[\"fy\", \"end\", \"filed\", \"val\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25b7d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Company-level computation\n",
    "# ----------------------------\n",
    "\n",
    "'''\n",
    "From a companyfacts JSON, build a fiscal-year table with:\n",
    "    revenue, shares, sales_per_share, sps_5y_cagr\n",
    "'''\n",
    "def compute_sales_per_share_from_companyfacts(cf: dict) -> pd.DataFrame:\n",
    "    usgaap = cf.get(\"facts\", {}).get(\"us-gaap\", {})\n",
    "    if not usgaap:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # revenue\n",
    "    rev_tag, rev_raw = choose_best_revenue_tag(usgaap, REVENUE_TAGS, unit=\"USD\")\n",
    "    if rev_tag is None or rev_raw is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rev_df = clean_annual_series(rev_raw, unit_name=f\"revenue:{rev_tag}\")\n",
    "    rev_df = rev_df.rename(columns={\"val\": \"revenue_usd\"})\n",
    "\n",
    "    # shares\n",
    "    if SHARES_TAG not in usgaap or \"units\" not in usgaap[SHARES_TAG] or \"shares\" not in usgaap[SHARES_TAG][\"units\"]:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    sh_raw = pd.DataFrame(usgaap[SHARES_TAG][\"units\"][\"shares\"])\n",
    "    sh_df  = clean_annual_series(sh_raw, unit_name=\"shares_outstanding\")\n",
    "    sh_df  = sh_df.rename(columns={\"val\": \"shares_outstanding\"})\n",
    "\n",
    "    # merge on fiscal year\n",
    "    merged = pd.merge(\n",
    "        rev_df[[\"fy\", \"filed\", \"revenue_usd\"]],\n",
    "        sh_df[[\"fy\", \"filed\", \"shares_outstanding\"]],\n",
    "        on=\"fy\",\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_rev\", \"_sh\")\n",
    "    )\n",
    "\n",
    "    # pick an effective date: max of the two filed dates (be conservative)\n",
    "    merged[\"effective_date\"] = merged[[\"filed_rev\", \"filed_sh\"]].max(axis=1)\n",
    "    merged = merged.drop(columns=[\"filed_rev\", \"filed_sh\"])\n",
    "\n",
    "    merged = merged.sort_values(\"fy\").reset_index(drop=True)\n",
    "\n",
    "    # compute sales per share\n",
    "    merged[\"sales_per_share\"] = merged[\"revenue_usd\"] / merged[\"shares_outstanding\"]\n",
    "    # 5-year CAGR of sales per share\n",
    "    merged[\"sales_per_share_lag5\"] = merged[\"sales_per_share\"].shift(5)\n",
    "    merged[\"sps_5y_cagr\"] = (merged[\"sales_per_share\"] / merged[\"sales_per_share_lag5\"]) ** (1/5) - 1\n",
    "\n",
    "    merged[\"revenue_tag_used\"] = rev_tag\n",
    "    merged = merged.drop(columns=[\"sales_per_share_lag5\"])\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "800237ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Batch runner\n",
    "# ----------------------------\n",
    "\n",
    "'''\n",
    "Loop through tickers → companyfacts → sales-per-share table.\n",
    "Returns:\n",
    "    - results_df (stacked fiscal-year rows)\n",
    "    - errors_df (ticker + reason)\n",
    "'''\n",
    "def build_sec_sales_growth_for_tickers(\n",
    "    tickers: List[str],\n",
    "    cik_map: pd.DataFrame,\n",
    "    headers: dict = SEC_HEADERS,\n",
    "    sleep_s: float = 0.2,\n",
    "    max_tickers: Optional[int] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    tickers = [t.upper() for t in tickers]\n",
    "    if max_tickers is not None:\n",
    "        tickers = tickers[:max_tickers]\n",
    "\n",
    "    # join to map\n",
    "    m = pd.merge(pd.DataFrame({\"ticker\": tickers}), cik_map, on=\"ticker\", how=\"left\")\n",
    "\n",
    "    results = []\n",
    "    errors = []\n",
    "\n",
    "    for i, row in m.iterrows():\n",
    "        ticker = row[\"ticker\"]\n",
    "        cik = row[\"cik\"]\n",
    "\n",
    "        if pd.isna(cik):\n",
    "            errors.append((ticker, \"no_cik\"))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            cf = get_companyfacts(cik, headers=headers)\n",
    "            tbl = compute_sales_per_share_from_companyfacts(cf)\n",
    "\n",
    "            if tbl.empty:\n",
    "                errors.append((ticker, \"missing_revenue_or_shares\"))\n",
    "                continue\n",
    "\n",
    "            tbl[\"ticker\"] = ticker\n",
    "            tbl[\"cik\"] = cik\n",
    "            results.append(tbl)\n",
    "\n",
    "            print(f\"[{i+1}/{len(m)}] OK {ticker} years={tbl['fy'].nunique()} tag={tbl['revenue_tag_used'].iloc[0]}\")\n",
    "        except Exception as e:\n",
    "            errors.append((ticker, f\"error:{str(e)[:150]}\"))\n",
    "            print(f\"[{i+1}/{len(m)}] FAIL {ticker} {e}\")\n",
    "\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "    results_df = pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "    errors_df = pd.DataFrame(errors, columns=[\"ticker\", \"reason\"])\n",
    "\n",
    "    return results_df, errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8028bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "daily_panel: MultiIndex (date, ticker)\n",
    "sec_fy_table: columns include ['ticker', 'effective_date', 'sps_5y_cagr'] (and maybe others)\n",
    "Returns daily_panel with a new column 'sec_sps_5y_cagr' forward-filled by ticker.\n",
    "'''\n",
    "def attach_sec_factor_daily(daily_panel: pd.DataFrame, sec_fy_table: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = daily_panel.copy()\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "    out[\"ticker\"] = out[\"ticker\"].astype(str).str.upper()\n",
    "    out = out.dropna(subset=[\"date\", \"ticker\"]).sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    sec = sec_fy_table.copy()\n",
    "    sec[\"ticker\"] = sec[\"ticker\"].astype(str).str.upper()\n",
    "    sec[\"effective_date\"] = pd.to_datetime(sec[\"effective_date\"], errors=\"coerce\")\n",
    "    sec = sec.dropna(subset=[\"ticker\", \"effective_date\", \"sps_5y_cagr\"])\n",
    "    sec = sec.sort_values([\"ticker\", \"effective_date\"]).reset_index(drop=True)\n",
    "\n",
    "    merged_list = []\n",
    "\n",
    "    for tkr, g in out.groupby(\"ticker\", sort=False):\n",
    "        s = sec.loc[sec[\"ticker\"] == tkr, [\"effective_date\", \"sps_5y_cagr\"]].copy()\n",
    "        if s.empty:\n",
    "            g = g.copy()\n",
    "            g[\"sec_sps_5y_cagr\"] = pd.NA\n",
    "            merged_list.append(g)\n",
    "            continue\n",
    "\n",
    "        # merge_asof needs same key name\n",
    "        s = s.rename(columns={\"effective_date\": \"date\"}).sort_values(\"date\")\n",
    "\n",
    "        g2 = pd.merge_asof(\n",
    "            g.sort_values(\"date\"),\n",
    "            s,\n",
    "            on=\"date\",\n",
    "            direction=\"backward\",\n",
    "            allow_exact_matches=True\n",
    "        )\n",
    "\n",
    "        # Create final output column cleanly (no duplicates)\n",
    "        g2 = g2.rename(columns={\"sps_5y_cagr\": \"sec_sps_5y_cagr\"})\n",
    "        merged_list.append(g2)\n",
    "\n",
    "    return pd.concat(merged_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fe817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/20] OK AA years=7 tag=RevenueFromContractWithCustomerExcludingAssessedTax\n",
      "[3/20] OK AAL years=7 tag=RevenueFromContractWithCustomerExcludingAssessedTax\n",
      "[5/20] OK AAPL years=9 tag=SalesRevenueNet\n",
      "[9/20] OK ACGL years=13 tag=Revenues\n",
      "[10/20] OK ACHC years=7 tag=RevenueFromContractWithCustomerExcludingAssessedTax\n",
      "[11/20] OK ACI years=2 tag=RevenueFromContractWithCustomerExcludingAssessedTax\n",
      "[12/20] OK ACM years=7 tag=RevenueFromContractWithCustomerExcludingAssessedTax\n",
      "[14/20] OK ADBE years=16 tag=Revenues\n",
      "[15/20] OK ADC years=14 tag=Revenues\n",
      "[16/20] OK ADI years=9 tag=SalesRevenueNet\n",
      "[18/20] OK ADP years=16 tag=Revenues\n",
      "[19/20] OK ADSK years=9 tag=SalesRevenueNet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>missing_revenue_or_shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAON</td>\n",
       "      <td>missing_revenue_or_shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>missing_revenue_or_shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABNB</td>\n",
       "      <td>missing_revenue_or_shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABT</td>\n",
       "      <td>missing_revenue_or_shares</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker                     reason\n",
       "0      A  missing_revenue_or_shares\n",
       "1   AAON  missing_revenue_or_shares\n",
       "2   ABBV  missing_revenue_or_shares\n",
       "3   ABNB  missing_revenue_or_shares\n",
       "4    ABT  missing_revenue_or_shares"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = pd.read_csv(\"ticker_list.csv\")[\"Ticker\"].dropna().tolist()\n",
    "\n",
    "cik_map = load_ticker_cik_map()\n",
    "\n",
    "results_df, errors_df = build_sec_sales_growth_for_tickers(\n",
    "    tickers=tickers, ## edit to include only midcap tickers!\n",
    "    cik_map=cik_map,\n",
    "    max_tickers=20,   # this is to limit the number of tickers, used for testing\n",
    "    sleep_s=0.25\n",
    ")\n",
    "\n",
    "results_df.head()\n",
    "errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c6e97ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maria\\AppData\\Local\\Temp\\ipykernel_17992\\3639332624.py:43: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(merged_list, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_with_growth_v1.0.csv\")\n",
    "daily_df = df.copy()\n",
    "daily_df[\"date\"] = pd.to_datetime(daily_df[\"date\"])\n",
    "daily_df[\"ticker\"] = daily_df[\"ticker\"].astype(str).str.upper()\n",
    "\n",
    "# Attach SEC factor (forward-fill based on effective_date)\n",
    "daily_with_sec = attach_sec_factor_daily(daily_df, results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data_with_growth_and_sec.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = \"data_with_growth_and_sec.csv\"\n",
    "daily_with_sec.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "694c1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"sec_sales_growth_fy_table.csv\", index=False)\n",
    "errors_df.to_csv(\"sec_sales_growth_errors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08265208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec_sps_5y_cagr\n",
      "False    999\n",
      "True      11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checks\n",
    "coverage = daily_df.groupby(\"ticker\")[\"sec_sps_5y_cagr\"].apply(lambda s: s.notna().any())\n",
    "print(coverage.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3204b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>fy</th>\n",
       "      <th>sales_per_share</th>\n",
       "      <th>sps_5y_cagr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>5.777230e+00</td>\n",
       "      <td>0.053504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.359886e+00</td>\n",
       "      <td>0.044808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.887191e+00</td>\n",
       "      <td>0.048104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.562899e+00</td>\n",
       "      <td>0.053655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.057807e+00</td>\n",
       "      <td>0.047118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>7.854850e+00</td>\n",
       "      <td>0.063368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>3.541515e+01</td>\n",
       "      <td>0.409769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>3.964984e+01</td>\n",
       "      <td>0.419188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>4.370832e+01</td>\n",
       "      <td>0.420284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>4.705366e+01</td>\n",
       "      <td>0.423231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ADP</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>5.073008e+01</td>\n",
       "      <td>0.452197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>ADSK</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>7.486675e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>ADSK</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.324670e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>ADSK</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2.621239e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>ADSK</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2.714222e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>ADSK</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2.587561e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>ADSK</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2.927753e+00</td>\n",
       "      <td>-0.171201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>ADSK</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2.889037e+00</td>\n",
       "      <td>0.044428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>ADSK</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.173400e+00</td>\n",
       "      <td>-0.939225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>ADSK</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2.536876e+00</td>\n",
       "      <td>-0.013423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker      fy  sales_per_share  sps_5y_cagr\n",
       "96     ADP  2015.0     5.777230e+00     0.053504\n",
       "97     ADP  2016.0     6.359886e+00     0.044808\n",
       "98     ADP  2017.0     6.887191e+00     0.048104\n",
       "99     ADP  2018.0     7.562899e+00     0.053655\n",
       "100    ADP  2019.0     8.057807e+00     0.047118\n",
       "101    ADP  2020.0     7.854850e+00     0.063368\n",
       "102    ADP  2021.0     3.541515e+01     0.409769\n",
       "103    ADP  2022.0     3.964984e+01     0.419188\n",
       "104    ADP  2023.0     4.370832e+01     0.420284\n",
       "105    ADP  2024.0     4.705366e+01     0.423231\n",
       "106    ADP  2025.0     5.073008e+01     0.452197\n",
       "107   ADSK  2009.0     7.486675e+00          NaN\n",
       "108   ADSK  2010.0     2.324670e+00          NaN\n",
       "109   ADSK  2011.0     2.621239e+06          NaN\n",
       "110   ADSK  2012.0     2.714222e+00          NaN\n",
       "111   ADSK  2013.0     2.587561e+00          NaN\n",
       "112   ADSK  2015.0     2.927753e+00    -0.171201\n",
       "113   ADSK  2016.0     2.889037e+00     0.044428\n",
       "114   ADSK  2017.0     2.173400e+00    -0.939225\n",
       "115   ADSK  2018.0     2.536876e+00    -0.013423"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks\n",
    "tickers_with_vals = coverage[coverage].index.tolist()\n",
    "print(\"Tickers with SEC factor:\", tickers_with_vals[:10])\n",
    "\n",
    "t = tickers_with_vals[0]\n",
    "daily_df.loc[daily_df[\"ticker\"] == t, [\"date\", \"sec_sps_5y_cagr\"]].dropna().tail(30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
